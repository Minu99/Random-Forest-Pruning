{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import seaborn as sns; sns.set() \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy as dcopy\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import explained_variance_score,r2_score,accuracy_score \n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices,_generate_sample_indices\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decalaration of initial file path for the results to be saved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterResultPath = './7_Cost_complexity_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Function**\n",
    "- To plot the size and accuracy raatio of the cost complexity pruned random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_scores(_models, plot_Values, tree_no, datasets_used, filesName ):\n",
    "            \n",
    "    size_matrix, acc_matrix, alpha_matrix=plot_Values\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xticks(range(len(_models)), _models)\n",
    "    figure, axs = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "    axs[0].boxplot(size_matrix.T, labels=_models, showmeans=True, meanline=True)\n",
    "    axs[0].set_ylabel('Ratio of the size')\n",
    "    axs[0].set_ylim([0, 1])\n",
    "    axs[1].boxplot(acc_matrix.T, labels=_models, showmeans=True, meanline=True)\n",
    "    axs[1].set_ylabel('Ratio of the accuracy')\n",
    "    axs[1].set_ylim([0, 2])\n",
    "    #plt.('Size and accuracy ratio of #decisiontrees = '+repr(trees_no) + ', Datasets : ' + datasets_used)\n",
    "    plt.savefig(filesName+'.png')\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "#     plt.figure()\n",
    "    \n",
    "#     plt.plot(alpha_matrix[0], label=r'$\\alpha^{\\ast}$'+'-DecisionTree')\n",
    "#     plt.plot(alpha_matrix[1], label=r'$\\alpha^{\\ast}$'+'-RandomForest')\n",
    "   \n",
    "    \n",
    "#     plt.ylabel('Cost-Complexity parameters :'+  r'$\\alpha^{\\ast}$')\n",
    "#     plt.title('Averaged Optimal Cost-Complexity parameters')\n",
    "#     plt.xlabel('Decision Tree index')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.savefig(filesName+'_alphas_Value'+'.png')\n",
    "#     plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions help to predict output of random forest for classification task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_randomForest(nodeScore_, testLeaves_Id):\n",
    "    tree_no = len(nodeScore_)\n",
    "    test_size = testLeaves_Id[0].shape[0]\n",
    "    classes_no = nodeScore_[0].shape[1]\n",
    "    probability_ValTest = np.zeros(shape=(test_size, classes_no))\n",
    "    for k in range(tree_no):\n",
    "        probability_ValTest += nodeScore_[k][testLeaves_Id[k], :]\n",
    "    probability_ValTest = probability_ValTest/tree_no\n",
    "    y_prediction_Forest = np.argmax(probability_ValTest, axis=1)\n",
    "    return y_prediction_Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prunes the initail decsion tree**\n",
    "- Helps to prune to optimal subtree using the optimum alpha obtained after cross validation using oob and training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optprunedTree(randomForest, testLeaves_id,opt_alpha_value,min_alpha_value_list):\n",
    "    trees_no = randomForest.n_estimators\n",
    "    opt_nl_ = [0]*trees_no\n",
    "    testLeaves_id_ = {}\n",
    "    \n",
    "    for k, estimator_ in enumerate(randomForest):\n",
    "        \n",
    "        testLeaves_id_[k] = testLeaves_id[k][opt_alpha_value[k]]\n",
    "        unprunedLeaves_id = testLeaves_id[k][min_alpha_value_list[k]]\n",
    "        orig_nl_ = len(np.unique(unprunedLeaves_id))\n",
    "        opt_nl_[k] = len(np.unique(testLeaves_id_[k]))/orig_nl_\n",
    "\n",
    "    return testLeaves_id_, np.mean(opt_nl_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out of Bag dataset prediction**\n",
    "- This method claculates the prediction of of oob sample \n",
    "- Input -Leaves id for each tree\n",
    "- Node score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forestOob(nodeScore_,outOfBag_indices, outOfBag_leaves_id,train_size):\n",
    "    trees_no = len(nodeScore_)\n",
    "    num_classes = nodeScore_[0].shape[1]\n",
    "    probOob_ = np.zeros(shape=(train_size, num_classes))\n",
    "    for t in range(trees_no):\n",
    "        probOob_[outOfBag_indices[t], :] += nodeScore_[t][outOfBag_leaves_id[t],:]\n",
    "    y_pred_outOfBa = np.argmax(probOob_, axis=1)\n",
    "    print(y_pred_outOfBa,\"---y_pred_outOfBa\")\n",
    "    return y_pred_outOfBa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global optimal cost complexity parameter method**\n",
    "- Function returns optimal gloabl cost complexity parameter alpha for all the decsion trees .\n",
    "- Possible  while globally pruning the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_globOptAlpha(randomForest, y_train, outOfBag_indices, alphaOutOfBag_leaves_id,alpha_value_list, nodeScore_):\n",
    "    uniqueAplha = []\n",
    "    print('')\n",
    "    print(alpha_value_list,\"---alpha_value_list\")\n",
    "    \n",
    "    #To convert into matrix format with n rows and columns\n",
    "    for k in alpha_value_list:\n",
    "        \n",
    "        uniqueAplha = uniqueAplha + alpha_value_list[k]\n",
    "        alpha_value_list[k] = np.array(alpha_value_list[k])\n",
    "        \n",
    "    uniqueAplha = np.array(uniqueAplha)\n",
    "    print('')\n",
    "    print(uniqueAplha,\"---uniqueAplha\")\n",
    "    \n",
    "    #Internal parameter so as to minimize alpha disscretization\n",
    "    numDecimals = 10\n",
    "    uniqueAplha = np.round(uniqueAplha, numDecimals)\n",
    "    uniqueAplha = np.unique(uniqueAplha)\n",
    "    trainSize = len(y_train)\n",
    "    \n",
    "    acc_ValueList_test = []\n",
    "    levelIndex = {}\n",
    "    \n",
    "    for k, estimator in enumerate(randomForest):\n",
    "        levelIndex[(0,k)] = 0\n",
    "    print(levelIndex,\"---levelIndex\") \n",
    "    \n",
    "    \n",
    "    #To return leave id value of threshold\n",
    "    def get_oobLeavesId_at_alpha(alp_thresh):       \n",
    "            \n",
    "        outOfBag_leaves_id = {}\n",
    "        alphaKeys = []\n",
    "        \n",
    "        for t_, estimator in enumerate(randomForest):\n",
    "            \n",
    "            alphaVal = alpha_value_list[t_][alpha_value_list[t_] <= alp_thresh]\n",
    "            \n",
    "            #When alphaVal is empty value it returns the largest alpha \n",
    "            if(sum(alphaVal)): \n",
    "                alpha_t_ = max(alphaVal)\n",
    "            \n",
    "            #Otherwise it retruns full tree ie lowest alpha  val\n",
    "            else:\n",
    "                alpha_t_ = min(alpha_value_list[t_])\n",
    "                \n",
    "            outOfBag_leaves_id[t_] = alphaOutOfBag_leaves_id[t_][alpha_t_]\n",
    "            alphaKeys.append(alpha_t_)\n",
    "            \n",
    "        return outOfBag_leaves_id, alphaKeys\n",
    "    \n",
    "    allAlphas = {}\n",
    "    \n",
    "    for k, alp in enumerate(uniqueAplha):\n",
    "        \n",
    "        outOfBag_leaves_id, allAlphas[k] = get_oobLeavesId_at_alpha(alp)\n",
    "        y_pred_outOfBag =predict_forestOob(nodeScore_,outOfBag_indices, outOfBag_leaves_id,trainSize)\n",
    "        acc_ = get_randomForest_acc(y_train, y_pred_outOfBag)\n",
    "        acc_ValueList_test.append(acc_)\n",
    "\n",
    "        \n",
    "    opt_alphaOob = allAlphas[np.argmax(acc_ValueList_test)]\n",
    "        \n",
    "    return opt_alphaOob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Method**\n",
    "- Used the tree for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTree(nodeScore_, leavesId):\n",
    "    p_val = nodeScore_[leavesId, :]\n",
    "    y_predicted_value = np.argmax(p_val, axis=1)\n",
    "    return y_predicted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimum alpha  in cost complexity pruning**\n",
    "- This method  calculatesthe  optimum alpha  value for list of  trees given dataset for crossvalidation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_optAlpha_(randomForest, outOfBag_leaves_id, y_train, alpha_value_list, outOfBag_indices, \n",
    "                              nodeScore_):\n",
    "    optimum_alpha = []\n",
    "    for k, estimator in enumerate(randomForest):\n",
    "        \n",
    "        testAcc_list = []\n",
    "        \n",
    "        for alp in alpha_value_list[k]:\n",
    "            \n",
    "            leavesId = outOfBag_leaves_id[k][alp]\n",
    "            y_predictValue = predictTree(nodeScore_[k], leavesId)\n",
    "            \n",
    "            print(k,estimator,\"---k,estimator\")\n",
    "            print(leavesId,\"---leavesId\")\n",
    "            print(y_predictValue,\"---y_predictValue\")\n",
    "            \n",
    "            #Get accuracy\n",
    "            accuracy_ = get_randomForest_acc(y_train[outOfBag_indices[k]], y_predictValue)\n",
    "            testAcc_list.append(accuracy_)\n",
    "       \n",
    "        print(testAcc_list,\"---testAcc_list\")\n",
    "        optimum_index = np.argmax(testAcc_list)\n",
    "        print(testAcc_list,\"---testAcc_list\")\n",
    "        optimum_alpha.append(alpha_value_list[k][optimum_index])\n",
    "        print(optimum_alpha,\"----optimum_alpha\")\n",
    "    \n",
    "    print('')    \n",
    "    print(optimum_alpha,\"----optimum_alpha\") \n",
    "    return optimum_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prune Internal Node**    \n",
    "- This method help to prune the  internal  node  coressponding to specified  node id both from left and right children list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_value_list(k, c_left, c_right):\n",
    "    cc_left = list(c_left)\n",
    "    cc_right = list(c_right)\n",
    "    stack_source = [k]\n",
    "    while len(stack_source)>0:\n",
    "        k = stack_source.pop()\n",
    "        if(cc_left[k]!=cc_right[k]):\n",
    "            stack_source.append(cc_left[k])\n",
    "            stack_source.append(cc_right[k])\n",
    "            cc_left[k] = -1\n",
    "            cc_right[k] = -1\n",
    "    return cc_left, cc_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Complexity parameter**\n",
    "- This method returns cost complexity parameter (alpha) corresponding to the nodes in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphaValue(impurity_rate, n_node_sample_values, internal_nodes_, c_left, c_right):\n",
    "    \n",
    "    alpha_value = {}\n",
    "    #print(internal_nodes_,\"---internal_nodes_\")\n",
    "    \n",
    "    #Iterate for each internal node in the tree\n",
    "    for node_ in internal_nodes_:\n",
    "        \n",
    "        leaf_node, __ = get_Leaves_below_node(node_, c_left, c_right)\n",
    "        #print(leaf_node,\"---leaf_node\")\n",
    "        RT = impurity_rate[node_] * n_node_sample_values[node_]\n",
    "        #print(RT,\"---RT\")\n",
    "        RTt = np.dot(impurity_rate[leaf_node], n_node_sample_values[leaf_node])\n",
    "        #print(RTt,\"---RTt\")\n",
    "        alpha_value[node_] = (RT - RTt)/ (len(leaf_node)-1)\n",
    "        \n",
    "    #print(alpha_value,\"---alpha_value\")  \n",
    "    return alpha_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaves at node n**\n",
    "- This method return the respected nodes and leaves below the specified node k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Leaves_below_node(k, c_left, c_right):\n",
    "    \n",
    "    #print(c_left,\"---c_left\")\n",
    "    #print(c_right,\"---c_right\")\n",
    "    stackspace = [k]\n",
    "    n_leaf = []\n",
    "    i_node = []\n",
    "    \n",
    "    while len(stackspace) > 0:\n",
    "        k = stackspace.pop()\n",
    "        #print(k,\"---k\")\n",
    "\n",
    "        if (c_left[k] != c_right[k]):\n",
    "            #print(c_left[k],\"---c_left[k]\")\n",
    "            #print(c_right[k],\"---c_right[k]\")\n",
    "            stackspace.append(c_left[k])\n",
    "            stackspace.append(c_right[k])\n",
    "            #print(stackspace,\"---stackspace\")\n",
    "            i_node.append(c_left[k])\n",
    "            i_node.append(c_right[k])\n",
    "            #print(i_node,\"---i_node\")\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            n_leaf.append(k) \n",
    "            \n",
    "        i_node = list(set(i_node)-set(n_leaf))\n",
    "    #print(i_node,\"---Final i_node\")\n",
    "    #print(n_leaf,\"---Final n_leaf\")\n",
    "    #Returns  all the leaf nodes and other nodes seperately\n",
    "    return n_leaf, i_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prune the tree**\n",
    "- Prunes current tree with the help of input score and id corresponding to each leaf\n",
    "        - Tree structure and tree leaves id are given as input\n",
    "        - Returns pruned tree leaves id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneTree_(treeParameters_, outOfBag_id, testId_):\n",
    "    \n",
    "    c_left, c_right, impurity_rate, n_node_sample_values=treeParameters_\n",
    "    \n",
    "    #To get all the nodes below 0th node\n",
    "    leaves_nodes, i_nodes_ = get_Leaves_below_node(0, c_left, c_right)\n",
    "    \n",
    "    #Initialization\n",
    "    alpha_value_list, node_value_list = [], []\n",
    "    all_outOfBag_id, all_testId_ = {}, {}\n",
    "    temp_root_outOfBag = dcopy(outOfBag_id)\n",
    "    temp_rootTest_ = dcopy(testId_)\n",
    "    \n",
    "    \n",
    "    while(len(i_nodes_)):\n",
    "            \n",
    "            #Calculates alpha value corresponding to each node\n",
    "            alpha_value = calculate_alphaValue(impurity_rate, n_node_sample_values, i_nodes_, c_left, c_right)\n",
    "            print(alpha_value,\"--- Returned alpha_value\")\n",
    "            \n",
    "            if(alpha_value):\n",
    "                \n",
    "                min_alphaNode = min(alpha_value, key=alpha_value.get)\n",
    "                print(min_alphaNode,\"---min_alphaNode\")\n",
    "                min_alpha_value = alpha_value[min_alphaNode]\n",
    "                print(min_alpha_value,\"---min_alpha_value\")\n",
    "                \n",
    "                alpha_value_list.append(min_alpha_value)\n",
    "                node_value_list.append(min_alphaNode)\n",
    "                \n",
    "                c_left_, c_right_ = prune_value_list(min_alphaNode, c_left, c_right)\n",
    "                leaves_nodes_, i_nodes_ = get_Leaves_below_node(0,c_left_, c_right_)\n",
    "                \n",
    "                #This section can be utilized to label the test samples\n",
    "                for lll in leaves_nodes_:\n",
    "                    old_leaves, _ = get_Leaves_below_node(lll,c_left, c_right)\n",
    "                    \n",
    "                    for leaf_ in old_leaves:\n",
    "                        outOfBag_id[outOfBag_id==leaf_] = lll\n",
    "                        testId_[testId_==leaf_] = lll\n",
    "                        \n",
    "                all_outOfBag_id[min_alpha_value] = dcopy(outOfBag_id)\n",
    "                all_testId_[min_alpha_value] = dcopy(testId_)\n",
    "                \n",
    "                c_left, c_right = c_left_, c_right_\n",
    "            else:\n",
    "                print('Done with pruning')\n",
    "                break\n",
    "                \n",
    "    #print(alpha_value,\"---alpha_value outside the loop\")\n",
    "    eps = 1e-10 #Internal value of the parameter\n",
    "    least_alpha_value = min(alpha_value_list) - eps\n",
    "    all_outOfBag_id[least_alpha_value] = temp_root_outOfBag\n",
    "    all_testId_[least_alpha_value] = temp_rootTest_\n",
    "    alpha_value_list = [least_alpha_value] + alpha_value_list\n",
    "    #print(alpha_value_list,\"---alpha_value_list\")\n",
    "    #print(all_outOfBag_id,\"---all_outOfBag_id\")\n",
    "    #print(all_testId_,\"---all_testId_\")\n",
    "    return alpha_value_list, all_outOfBag_id, all_testId_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to get class probabilities**\n",
    "- This method return class probabilities of all nodes at once   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_pb(estimator_tree):\n",
    "    #print(estimator_tree.tree_.value,\"---estimator_tree.tree_.value\")\n",
    "    #print(estimator_tree.tree_.value[:,0,:],\"---estimator_tree.tree_.value[:,0,:]\")\n",
    "    bin_cnt = estimator_tree.tree_.value[:,0,:]\n",
    "    #print(bin_cnt,\"---bin_cnt\")\n",
    "    bin_cnt /= bin_cnt.sum(axis=1)[:,np.newaxis]\n",
    "    return bin_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Function**\n",
    "- To get the impurity of each node in estimator tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errorFun_(estimator_tree):   \n",
    "    return estimator_tree.tree_.impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplha Sequence Calculator**\n",
    "- Calculates the aplha value sequence with respect to the nested trees in each random forest.\n",
    "- Return value-\n",
    "        -Aplha_value_list: A list of alpha values  corresponding to each estimator\n",
    "        -Training accuracy after each set of alpha value\n",
    "        -List of nodes to be pruned according to alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_value(randomForest, X_train, X_test, outOfBag_indices):\n",
    "    \n",
    "    nodeScore_, outOfBag_leaves_id, alpha_value_list = {}, {}, {}\n",
    "    tree_no = randomForest.n_estimators\n",
    "    min_alpha_value_list = {}\n",
    "    testLeaves_id = {}\n",
    "    \n",
    "    for treeno in range(tree_no):\n",
    "        estimator_tree = randomForest.estimators_[treeno]\n",
    "        \n",
    "        #To get the child nodes  of tthe decsion tree\n",
    "        c_left = np.array(estimator_tree.tree_.children_left, dtype=int)\n",
    "        c_right = np.array(estimator_tree.tree_.children_right, dtype=int)\n",
    "        \n",
    "        X_outofBag_tree = X_train[outOfBag_indices[treeno], :].view()\n",
    "        #print(X_outofBag_tree,\"---X_outofBag_tree\")\n",
    "\n",
    "        #To get the error function of the estimator to get the impurity    \n",
    "        impurity_rate = get_errorFun_(estimator_tree)\n",
    "        print(impurity_rate,\"---impurity_rate\")\n",
    "        \n",
    "        n_node_sample_values = estimator_tree.tree_.n_node_samples/estimator_tree.tree_.n_node_samples[0]\n",
    "        #print(estimator_tree.tree_.n_node_samples,\"---estimator_tree.tree_.n_node_samples\")\n",
    "        #print(estimator_tree.tree_.n_node_samples[0],\"---estimator_tree.tree_.n_node_samples[0]\")\n",
    "        #print(n_node_sample_values,\"---n_node_sample_values\")\n",
    "        \n",
    "        #To get the probabilities of all node at once\n",
    "        nodeScore_[treeno] = get_class_pb(estimator_tree)\n",
    "        #print(nodeScore_[treeno],\"---nodeScore_[treeno]\")\n",
    "       \n",
    "        #Return the index of the leaf  for Out of bag sample set ends up in \n",
    "        outOfBag_id = estimator_tree.apply(X_outofBag_tree)\n",
    "        print(outOfBag_id,\"---outOfBag_id\")\n",
    "         \n",
    "        #Return the index of the leaf  for test sample set ends up in \n",
    "        testId_ = estimator_tree.apply(X_test)\n",
    "        print(testId_,\"---testId_\")\n",
    "        \n",
    "        treeParameters_ = c_left, c_right, impurity_rate, n_node_sample_values\n",
    "        \n",
    "        #To get the minimum value for alpha \n",
    "        alpha_value_list[treeno], outOfBag_leaves_id[treeno], testLeaves_id[treeno] = pruneTree_(treeParameters_, outOfBag_id, testId_)\n",
    "        #print(alpha_value_list[treeno],\"---alpha_value_list[treeno]\")\n",
    "        #print(outOfBag_leaves_id[treeno],\"---outOfBag_leaves_id[treeno]\")\n",
    "        #print(testLeaves_id[treeno],\"---testLeaves_id[treeno]\")\n",
    "        min_alpha_value_list[treeno] = min(alpha_value_list[treeno])\n",
    "        #print(min_alpha_value_list[treeno],\"---min_alpha_value_list[treeno]\")\n",
    "        \n",
    "    return (alpha_value_list, outOfBag_leaves_id, testLeaves_id, nodeScore_, min_alpha_value_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to get Out Of Bag indices for each estimator in random forest**\n",
    "- This method returns a dictionary where key indicates estimator and value indicates the out of bag indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outOfBag_indices(randomForest, X_train):\n",
    "    outOfBag_indices = {}\n",
    "    for k, _estimator in enumerate(randomForest):\n",
    "        #print(k,\"k\")\n",
    "        outOfBag_indices[k] = _generate_unsampled_indices(_estimator.random_state,X_train.shape[0])\n",
    "        #print(X_train.shape[0],\"X_train.shape[0]\")\n",
    "        #print(outOfBag_indices[k],\"outOfBag_indices[k]\")\n",
    "    #print(outOfBag_indices)\n",
    "    return outOfBag_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to claculates the accuracy**\n",
    "- Return accuracy score of random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randomForest_acc(y_test, y_predict):\n",
    "    rf_acc = accuracy_score(y_test, y_predict)\n",
    "    return rf_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function**\n",
    "- Helper Method is helps to run all the iterations in parralel loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_parallel_fuction(arguments):\n",
    "    evaluating_func = arguments[0]\n",
    "    evaluating_func_args = arguments[1:]\n",
    "    return evaluating_func(*evaluating_func_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Complexity Pruning**\n",
    "- Prunes the random forest using out of bag indices with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costComplexityPruning_withCv(randomForest, data_for_processing, CV_set='OOB-set'):    \n",
    "    \n",
    "    #Data set\n",
    "    X_train, X_test, y_train, y_test = data_for_processing\n",
    "    #print(\"X_train\",X_train,\"X_test\", X_test,\"y_train\", y_train,\"y_test\", y_test)\n",
    "    \n",
    "    #To get the accuracy scor of the original random forest\n",
    "    randomForest_acc = get_randomForest_acc(y_test, randomForest.predict(X_test))\n",
    "    print(randomForest_acc,\"---randomForest_acc\")\n",
    "    \n",
    "    #To get the out of the bag indices\n",
    "    outOfBag_indices = get_outOfBag_indices(randomForest, X_train)\n",
    "    print(outOfBag_indices,\"---outOfBag_indices\")\n",
    "    \n",
    "    #To get potential alpha value \n",
    "    alpha_param = get_alpha_value(randomForest, X_train, X_test, outOfBag_indices)\n",
    "    print(alpha_param,\"---alpha_param\")\n",
    "    alpha_value_list, outOfBag_leaves_id, testLeaves_id, nodeScore_, min_alpha_value_list = alpha_param \n",
    "    \n",
    "    #To get Cross validation error for estimator by setting the optimum alpha[t] value\n",
    "    opt_alpha_value = get_optAlpha_(randomForest, outOfBag_leaves_id, y_train, alpha_value_list, outOfBag_indices, \n",
    "                              nodeScore_)   \n",
    "        \n",
    "    #To get Cross validation error for forest by setting the optimum alpha value gloablly\n",
    "    opt_alphaOob = get_globOptAlpha(randomForest, y_train, outOfBag_indices, outOfBag_leaves_id, \n",
    "                                       alpha_value_list, nodeScore_)\n",
    "    print(opt_alphaOob,\"---opt_alphaOob\")\n",
    "      \n",
    "    #Prune the tree upto reaching optimal aplha value\n",
    "    optiTestId, pruneRatio = get_optprunedTree(randomForest, testLeaves_id,opt_alpha_value,min_alpha_value_list)\n",
    "    yPrune = predict_randomForest(nodeScore_, optiTestId)\n",
    "    accuracy_test_pruned = get_randomForest_acc(y_test, yPrune)\n",
    "    print(accuracy_test_pruned,\"---accuracy_test_pruned\")\n",
    "    \n",
    "    #Prune the Forest upto reaching optimal aplha value\n",
    "    optiTestIdOob, pruneRatioOob = get_optprunedTree(randomForest, testLeaves_id,opt_alphaOob,min_alpha_value_list)\n",
    "    y_pruneOob = predict_randomForest(nodeScore_, optiTestIdOob)\n",
    "    accuracy_test_pruned_oob = get_randomForest_acc(y_test, y_pruneOob)\n",
    "    print(accuracy_test_pruned_oob,\"---accuracy_test_pruned_oob\") \n",
    "    \n",
    "    \n",
    "    #To plot the alpha value\n",
    "    def plot_alpha_value_lists():\n",
    "        plt.figure()\n",
    "        for k in alpha_value_list:\n",
    "            plt.plot(alpha_value_list[k])\n",
    "        plt.xlabel('No: of unique' +r'$\\alpha value \\in \\mathcal{A}_j$')\n",
    "        plt.ylabel('Cost-complexity param' + r'$\\mathcal{A}_j$')\n",
    "        plt.show()\n",
    "        \n",
    "    #plot_alpha_value_lists()\n",
    "    \n",
    "    \n",
    "    #Ratio between pruned aand unpruned decsion tree\n",
    "    accuracy_testRatio = accuracy_test_pruned/randomForest_acc\n",
    "    accuracy_testRatio_oob = accuracy_test_pruned_oob/randomForest_acc\n",
    "    print(accuracy_testRatio ,\"---accuracy_testRatio  using test sample\")\n",
    "    print(accuracy_testRatio_oob,\"---accuracy_testRatio_oob using oob sample\")\n",
    "    \n",
    "    \n",
    "    Finaloutput_array = np.array([pruneRatio, accuracy_testRatio, pruneRatioOob,\n",
    "                             accuracy_test_pruned_oob,randomForest_acc])\n",
    "    alpha_values = np.concatenate((opt_alpha_value, opt_alphaOob))\n",
    "    \n",
    "    return np.concatenate((Finaloutput_array, alpha_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize and fit random forest classifier with the training dataset**\n",
    "- Bootstrap  is initialized as true and this method requires model name and dataset as input parameters\n",
    "- Random forest classifier with n number of estimators is the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randomforest(model_name, tree_no, data_set):\n",
    "    X_train, X_test, y_train, y_test = data_set\n",
    "    randomForest = RandomForestClassifier(bootstrap=True,n_estimators = tree_no)\n",
    "    randomForest.set_params()\n",
    "    randomForest.fit(X_train, y_train)\n",
    "#     print(randomForest)\n",
    "    return randomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To read the dataset from the datasource**\n",
    "- No input parameters required\n",
    "- Outputs data in the form of attributtes and class form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irisdata():\n",
    "    irisdata = datasets.load_iris()\n",
    "    X = irisdata.data  \n",
    "    y = irisdata.target\n",
    "#     print(X,y,\"---X,y\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method calls the method to get the dataaset**\n",
    "- Multiple dataset are retrieved from this method. Required input is the name of indented dataset\n",
    "- Outputs dataset that are ready for test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "#     print(name,\"---name\")\n",
    "    dataset_method = {\"iris_dataset\" : get_irisdata()}  \n",
    "#     print(dataset_method[name],\"---dataset_method[name]\")\n",
    "    return dataset_method[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splits the dataset into test and train data**\n",
    "- Input required- The split size of the test data and the dataset to be split\n",
    "- Output- Provides 2 pair of dataset - Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_set, split_size):\n",
    "    X, y = get_dataset(data_set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=split_size)\n",
    "    data_for_processing = X_train, X_test, y_train, y_test\n",
    "#     print(data_for_processing,\"---data_for_processing\")\n",
    "    return data_for_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methof to specify Split Size**\n",
    "- This method specifies the splt size of each dataset\n",
    "- Outputs split size along with the dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_size(dataset_name):\n",
    "    test_size={'iris_dataset': 0.25}\n",
    "#     print(test_size,\"---test_size\")\n",
    "    return test_size[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test the random forest classifier on out of bag set and train set**\n",
    "- Both these oob and train dataset are considered as cross validation set and compared\n",
    "- Required Inputs- Cross validation set i.e out of bag dataset and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cc_prune(iter_no,test_models,datasets_used,tree_no,CV_set='OOB-set'):\n",
    "    \n",
    "    #To create the Folders if it doesnt exist\n",
    "    if not os.path.exists(iterResultPath):\n",
    "        os.makedirs(iterResultPath)\n",
    "        \n",
    "    #To open the text file for writing the results obtained\n",
    "    stdout_ = sys.stdout\n",
    "    file = open('./7_Cost_complexity_results/Results__.txt', 'w')\n",
    "    sys.stdout = file    \n",
    "        \n",
    "    #Iterate through each dataset\n",
    "    for data_set in datasets_used:        \n",
    "        \n",
    "        #To get the test train split ratio of each dataset\n",
    "        print(data_set,\"---data_set\")\n",
    "        split_size = get_split_size(data_set)        \n",
    "        print(split_size,\"---split_size\")\n",
    "        \n",
    "        #Initializing matrix\n",
    "        size_matrix = np.zeros(shape=(len(test_models)*2,iter_no))\n",
    "        acc_matrix = np.zeros(shape=(len(test_models)*2,iter_no))\n",
    "        rf_matrix = np.zeros(shape=(len(test_models)*2,iter_no))\n",
    "        alpha_matrix = np.zeros(shape=(len(test_models)*2,iter_no, tree_no))\n",
    "        print(size_matrix,\"---size_matrix\")\n",
    "        print(acc_matrix,\"---acc_matrix\")\n",
    "        print(alpha_matrix,\"---alpha_matrix\")\n",
    "        \n",
    "        i = 0\n",
    "        _models = []\n",
    "        output_dict = {}\n",
    "        for m, model in enumerate(test_models):\n",
    "            print(test_models,\"---test_models\")\n",
    "            print(m,\"---m\")\n",
    "            print(model,\"----model\")\n",
    "            argument_list = []\n",
    "            for k in range(iter_no):\n",
    "                #Get the data as train and test dataset for further processing\n",
    "                data_for_processing = get_data(data_set, split_size)\n",
    "                #Create Random forest classifier object using the dataset and given parameters\n",
    "                randomForest = get_randomforest(model, tree_no, data_for_processing)                          \n",
    "                #To create Tuple to run parallel\n",
    "                argumentTuple = (costComplexityPruning_withCv,randomForest, data_for_processing, CV_set)\n",
    "                argument_list.append(argumentTuple)         \n",
    "                \n",
    "#                 for arguments in argument_list:            \n",
    "#                     evaluating_func = arguments[0]                    \n",
    "#                     evaluating_func_args = arguments[1:]\n",
    "#                     results_of_dataset=evaluating_func(*evaluating_func_args)\n",
    "                \n",
    "            results_of_dataset = Parallel(n_jobs=4)(delayed(helper_parallel_fuction)(arguments) for arguments in argument_list)\n",
    "            print(results_of_dataset,\"---results_of_dataset\")\n",
    "            results_matrix = np.array(results_of_dataset) \n",
    "            print(results_matrix,\"---results_matrix\")\n",
    "\n",
    "            size_matrix[i,:] = results_matrix[:,0]\n",
    "            acc_matrix[i,:] = results_matrix[:,1]\n",
    "            \n",
    "            \n",
    "            value1_mean = \"{0:.3f}\".format(results_matrix[:,0].mean())\n",
    "            value1_std =  \"{0:.3f}\".format(results_matrix[:,0].std())\n",
    "            value1 =  value1_mean + '+/-' + value1_std\n",
    "            print(value1,\"--Size -value1\")\n",
    "            \n",
    "            value2_mean = \"{0:.3f}\".format(results_matrix[:,1].mean()) \n",
    "            value2_std = \"{0:.3f}\".format(results_matrix[:,1].std()) \n",
    "            value2 = value2_mean + '+/-' + value2_std\n",
    "            print(value2,\"---Accuracy value2\")\n",
    "            \n",
    "            \n",
    "            output_dict[model+'-DecsionTree'] = value1 +',   '+ value2\n",
    "            print(output_dict,\"---output_dict\")\n",
    "            \n",
    "            \n",
    "            alpha_matrix[i,:,:] = results_matrix[:,5:5+tree_no]\n",
    "            print(alpha_matrix,\"---alpha_matrix\")\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "            _models.append(model+'\\n'+'DecsionTree')\n",
    "            print(_models,\"---_models\")\n",
    "            \n",
    "            size_matrix[i,:] = results_matrix[:,2]\n",
    "            acc_matrix[i,:] = results_matrix[:,3]\n",
    "            \n",
    "            rf_matrix[i,:]= results_matrix[:,4]\n",
    "            print(rf_matrix,\"---rf_matrix\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            value1_mean = \"{0:.3f}\".format(results_matrix[:,2].mean()) \n",
    "            value1_std = \"{0:.3f}\".format(results_matrix[:,2].std())\n",
    "            value1 = value1_mean + '+/-' + value1_std\n",
    "            print(value1,\"---Size value1\")\n",
    "            \n",
    "            value2_mean = \"{0:.3f}\".format(results_matrix[:,3].mean()) \n",
    "            value2_std = \"{0:.3f}\".format(results_matrix[:,3].std()) \n",
    "            value2 = value2_mean + '+/-' + value2_std\n",
    "            print(value2,\"---Accuracy value2\")\n",
    "            \n",
    "            output_dict[model+'-RandomForest'] = value1 +',   '+ value2\n",
    "            print(output_dict,\"---Second output_dict\")\n",
    "            \n",
    "            alpha_matrix[i,:,:] = results_matrix[:,5+tree_no::]\n",
    "            print(alpha_matrix,\"---alpha_matrix\")\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "            _models.append(model+'\\n'+'OOB')\n",
    "            print(_models,\"---Second _models\")\n",
    "\n",
    "        plot_Values = size_matrix, acc_matrix, alpha_matrix\n",
    "        print(size_matrix,\"---size_matrix\")\n",
    "        print(acc_matrix,\"---acc_matrix\")\n",
    "        print(alpha_matrix,\"---alpha_matrix\")\n",
    "        \n",
    "        print('')\n",
    "        print(datasets_used)\n",
    "        print('-----------------------------------------------')\n",
    "        for k in output_dict:\n",
    "            print(k + '==> Size, Accuracy ratio  : ' + output_dict[k])             \n",
    "\n",
    "    sys.stdout = stdout_\n",
    "    file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cc_prune_param={'test_models' : ['RandomForestClassifier'],\n",
    "                                 'datasets_used' : ['iris_dataset'],\n",
    "                                 'iter_no' : 1,\n",
    "                                 'tree_no' : 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_cc_prune_param)\n",
    "test_cc_prune(**test_cc_prune_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
